# -*- coding: utf-8 -*-
"""simultaneousVSsequential - hyperparameter search

Grid search implementation for PINN hyperparameters
"""

import os
import time
import json
import copy

import csv
import torch
import matplotlib.pyplot as plt

# Initialize empty arrays for each column
numerical_t = []
numerical_u1 = []
numerical_u2 = []
numerical_u3 = []

# Open the CSV file
with open('DATA.csv', 'r') as csvfile:
    # Create a CSV reader object
    csvreader = csv.reader(csvfile)

    # Iterate over each row in the CSV file
    for row in csvreader:
        # Assuming the CSV file has four columns
        # Append each value to the corresponding array
        numerical_t.append([float(row[0])])
        numerical_u1.append([float(row[1])])
        numerical_u2.append([float(row[2])])
        numerical_u3.append([float(row[3])])

numerical_u1 = torch.tensor(numerical_u1)
numerical_u2 = torch.tensor(numerical_u2)
numerical_u3 = torch.tensor(numerical_u3)

def plot_u1(x_p, x_b_0, x_b_1, x_t, u, output_label):
    plt.figure(figsize=(6, 2.5))
    plt.scatter(x_p.detach()[:, 0], torch.zeros_like(x_p)[:, 0], s=20, lw=0, color="tab:green", alpha=0.6)
    plt.scatter(x_b_0.detach()[:, 0], torch.zeros_like(x_b_0)[:, 0], s=20, lw=0, color="tab:red", alpha=0.6)
    plt.scatter(x_b_1.detach()[:, 0], torch.zeros_like(x_b_1)[:, 0], s=20, lw=0, color="tab:red", alpha=0.6)
    plt.plot(x_t[:, 0], u, label=output_label, color="tab:blue", alpha=0.6)
    plt.plot(x_t[:, 0], numerical_u1,label="u1_numerical", color="tab:orange", alpha=0.6 )
    plt.title(output_label)
    plt.legend()
    plt.show()

def plot_u2(x_p, x_b_0, x_b_1, x_t, u, output_label):
    plt.figure(figsize=(6, 2.5))
    plt.scatter(x_p.detach()[:, 0], torch.zeros_like(x_p)[:, 0], s=20, lw=0, color="tab:green", alpha=0.6)
    plt.scatter(x_b_0.detach()[:, 0], torch.zeros_like(x_b_0)[:, 0], s=20, lw=0, color="tab:red", alpha=0.6)
    plt.scatter(x_b_1.detach()[:, 0], torch.zeros_like(x_b_1)[:, 0], s=20, lw=0, color="tab:red", alpha=0.6)
    plt.plot(x_t[:, 0], u, label=output_label, color="tab:blue", alpha=0.6)
    plt.plot(x_t[:, 0], numerical_u2,label="u2_numerical", color="tab:orange", alpha=0.6 )
    plt.title(output_label)
    plt.legend()
    plt.show()

def plot_u3(x_p, x_b_0, x_b_1, x_t, u, output_label):
    plt.figure(figsize=(6, 2.5))
    plt.scatter(x_p.detach()[:, 0], torch.zeros_like(x_p)[:, 0], s=20, lw=0, color="tab:green", alpha=0.6)
    plt.scatter(x_b_0.detach()[:, 0], torch.zeros_like(x_b_0)[:, 0], s=20, lw=0, color="tab:red", alpha=0.6)
    plt.scatter(x_b_1.detach()[:, 0], torch.zeros_like(x_b_1)[:, 0], s=20, lw=0, color="tab:red", alpha=0.6)
    plt.plot(x_t[:, 0], u, label=output_label, color="tab:blue", alpha=0.6)
    plt.plot(x_t[:, 0], numerical_u3,label="u3_numerical", color="tab:orange", alpha=0.6 )
    plt.title(output_label)
    plt.legend()
    plt.show()

def plot_output(x_p, x_b_0, x_b_1, x_t, u, output_label):
    plt.figure(figsize=(6, 2.5))
    plt.scatter(x_p.detach()[:, 0], torch.zeros_like(x_p)[:, 0], s=20, lw=0, color="tab:green", alpha=0.6)
    plt.scatter(x_b_0.detach()[:, 0], torch.zeros_like(x_b_0)[:, 0], s=20, lw=0, color="tab:red", alpha=0.6)
    plt.scatter(x_b_1.detach()[:, 0], torch.zeros_like(x_b_1)[:, 0], s=20, lw=0, color="tab:red", alpha=0.6)
    plt.plot(x_t[:, 0], u, label=output_label, color="tab:blue", alpha=0.6)
    plt.plot(x_t[:, 0], numerical_u3,label=output_label, color="tab:blue", alpha=0.6 )
    plt.title(output_label)
    plt.legend()
    plt.show()

def plot_loss(loss, string):
  plt.figure(figsize=(6, 2.5))
  plt.plot(loss)
  plt.title(string)
  plt.show()

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt


class FCN(nn.Module):
    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):
        super().__init__()
        activation = nn.Tanh

        self.fcs = nn.Sequential(
            nn.Linear(N_INPUT, N_HIDDEN),
            activation()
        )

        self.fch = nn.Sequential(*[
            nn.Sequential(
                nn.Linear(N_HIDDEN, N_HIDDEN),
                activation()
            ) for _ in range(N_LAYERS - 1)
        ])

        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)

        self._init_weights()

    def _init_weights(self):
        # Initialize weights with Xavier initialization
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        x = self.fcs(x)
        x = self.fch(x)
        x = self.fce(x)
        return x



# define boundary points, for the boundary loss
x_b_0 = torch.tensor(0.).view(-1,1).requires_grad_(True)
x_b_1 = torch.tensor(1.).view(-1,1).requires_grad_(True)
# define training points over the entire domain, for the physics loss
x_p = torch.linspace(0,1,301).view(-1,1).requires_grad_(True)
#test points
x_t = torch.linspace(0,1,301).view(-1,1)

pinn1 = FCN(1,1,32,4)
pinn2 = FCN(1,1,32,4)
pinn3 = FCN(1,1,32,4)

optimiser1 = torch.optim.Adam(pinn1.parameters(),lr=1e-3)
optimiser2 = torch.optim.Adam(pinn2.parameters(),lr=1e-3)
optimiser3 = torch.optim.Adam(pinn3.parameters(),lr=1e-3)
lambdas = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]
d =0.01

i=0
u1_physics = pinn1(x_p) #(300,1)
du1dx = torch.autograd.grad(u1_physics, x_p, torch.ones_like(u1_physics), create_graph=True)[0]
u2_physics = pinn1(x_p) #(300,1)
du2dx = torch.autograd.grad(u2_physics, x_p, torch.ones_like(u2_physics), create_graph=True)[0]
u3_physics = pinn1(x_p)
du3dx = torch.autograd.grad(u2_physics, x_p, torch.ones_like(u2_physics), create_graph=True)[0]


physics_u1s = []
physics_u2s = []
physics_u3s = []
boundary_u1s = []
boundary_u2s = []
u1u2 = []
u1u3 = []
while i<=80000:
  #u1
  for j in range(30):
    optimiser1.zero_grad()
    u1_b_0 = pinn1(x_b_0)# (1, 1)
    u1_b_1 = pinn1(x_b_1)# (1, 1)
    u1_physics = pinn1(x_p) #(300,1)
    du1dx = torch.autograd.grad(u1_physics, x_p, torch.ones_like(u1_physics), create_graph=True)[0]

    u1_loss1 = (torch.squeeze(u1_b_0) - 1)**2
    u1_loss2 = torch.mean((u1_physics* du2dx.detach() + u2_physics.detach() * du1dx)**2)
    u1_loss3 =  torch.mean((u3_physics.detach() *du1dx + u1_physics*du3dx.detach() )**2)
    physics_loss_u1 = u1_loss3 + u1_loss2
    boundary_loss_u1 = u1_loss1

    u1_loss = lambdas[0]*u1_loss1 + lambdas[3]*(u1_loss2 + u1_loss3)
    u1_loss.backward()
    optimiser1.step()

    boundary_u1s.append(boundary_loss_u1.detach())
    physics_u1s.append(physics_loss_u1.detach())
    u1u2.append(torch.mean((u1_physics * u2_physics)).detach())
    i+=1
  #u2
  for k in range(30):
    optimiser2.zero_grad()
    u2_b_0 = pinn2(x_b_0)# (1, 1)
    u2_b_1 = pinn2(x_b_1)# (1, 1)
    u2_physics = pinn2(x_p) #(300,1)
    du2dx = torch.autograd.grad(u2_physics, x_p, torch.ones_like(u2_physics), create_graph=True)[0]

    u2_loss1 = (torch.squeeze(u2_b_0) - 1)**2
    u2_loss2 = (torch.squeeze(u2_b_1) - 20)**2
    u2_loss3 =  torch.mean((u1_physics.detach() * du2dx + u2_physics * du1dx.detach())**2)
    u2_loss4 =  torch.mean((u2_physics *du3dx.detach() - 2 * (u3_physics.detach() + 1/d)*du2dx + (1/d)*u3_physics.detach())**2)
    physics_loss_u2 = u2_loss3 + u2_loss4
    boundary_loss_u2 = u2_loss1 +  u2_loss2

    u2_loss = lambdas[0]*(u2_loss1+u2_loss2) + lambdas[3]*u2_loss3 + lambdas[3] * u2_loss4
    u2_loss.backward()
    optimiser2.step()

    boundary_u2s.append(boundary_loss_u2.detach())
    physics_u2s.append(physics_loss_u2.detach())
    u1u2.append(torch.mean((u1_physics * u2_physics)).detach())
    u1u3.append(torch.mean((u1_physics * u3_physics)).detach())
    i+=1
  #u3
  for l in range(30):
    optimiser3.zero_grad()
    u3_physics = pinn3(x_p)
    du3dx = torch.autograd.grad(u3_physics, x_p, torch.ones_like(u3_physics), create_graph=True)[0]

    u3_loss1 =  torch.mean((u3_physics*du1dx.detach() + u1_physics.detach()*du3dx )**2)
    u3_loss2 =  torch.mean((u2_physics.detach() *du3dx - 2 * (u3_physics + 1/d)*du2dx.detach() + (1/d)*u3_physics)**2)
    physics_loss_u3 = u3_loss1 + u3_loss2

    u3_loss = lambdas[3] * u3_loss1 + lambdas[7] * u3_loss2
    u3_loss.backward()
    optimiser3.step()

    physics_u3s.append(physics_loss_u3.detach())
    i+=1

  if i%900 == 0:
    pass

#plot u1,u2,u3
u1 = pinn1(x_t).detach()
u2 = pinn2(x_t).detach()
u3 = pinn3(x_t).detach()
plot_u1(x_p, x_b_0, x_b_1, x_t, u1, "u1 at iteration 40000")#)+ " for lambada = " + str(lambda1
plot_u2(x_p, x_b_0, x_b_1, x_t, u2, "u2 at iteration 40000")#+ " for lambada = " + str(lambda1))
plot_u3(x_p, x_b_0, x_b_1, x_t, u3, "u3 at iteration 40000") #+ " for lambada = " + str(lambda1))

'''
#plot du1, du2, du3
plot_output(x_p, x_b_0, x_b_1, x_t, du1dx.detach(), "Derivative of Output 1 (u1) at iteration" + str(i))# + " for lambada = " + str(lambda1))
plot_output(x_p, x_b_0, x_b_1, x_t, du2dx.detach(), "Derivative of Output 1 (u2) at iteration" + str(i)) # + " for lambada = " + str(lambda1))
plot_output(x_p, x_b_0, x_b_1, x_t, du3dx.detach(), "Derivative of Output 1 (u3) at iteration" + str(i)) #+ " for lambada = " + str(lambda1))
'''


plot_loss(physics_u1s, "physics loss for u1")
plot_loss(physics_u2s, "physics loss for u2")
plot_loss(physics_u3s, "physics loss for u3")
plot_loss(boundary_u1s, "boundary loss for u1")
plot_loss(boundary_u2s, "boundary loss for u2")
plot_loss(u1u2, "u1*u2")
plot_loss(u1u3, "u1*u3")
print(pinn3(x_b_0), " value of u3(0)")

mse_u1 = torch.mean(((numerical_u1) - (u1_physics)) ** 2).detach()
mse_u2 = torch.mean(((numerical_u2) - (u2_physics)) ** 2).detach()
mse_u3 = torch.mean(((numerical_u3) - (u3_physics)) ** 2).detach()

# Plot MSE for each variable
variables = ['u1', 'u2', 'u3']
mse_values = [mse_u1, mse_u2, mse_u3]

print(mse_values)
print(sum(mse_values))
plot_u1(x_p, x_b_0, x_b_1, x_t, u1, "u1 at iteration 80000")#)+ " for lambada = " + str(lambda1
plot_u2(x_p, x_b_0, x_b_1, x_t, u2, "u2 at iteration 80000")#+ " for lambada = " + str(lambda1))
plot_u3(x_p, x_b_0, x_b_1, x_t, u3, "u3 at iteration 80000") #+ " for lambada = " + str(lambda1))
plot_loss(physics_u1s, "")
plot_loss(physics_u2s, "")
plot_loss(physics_u3s, "")

"""0.1"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt


# define boundary points, for the boundary loss
x_b_0 = torch.tensor(0.).view(-1,1).requires_grad_(True)
x_b_1 = torch.tensor(1.).view(-1,1).requires_grad_(True)
# define training points over the entire domain, for the physics loss
x_p = torch.linspace(0,1,301).view(-1,1).requires_grad_(True)
#test points
x_t = torch.linspace(0,1,301).view(-1,1)

pinn1 = FCN(1,1,32,4)
pinn2 = FCN(1,1,32,4)
pinn3 = FCN(1,1,32,4)

optimiser1 = torch.optim.Adam(pinn1.parameters(),lr=1e-3)
optimiser2 = torch.optim.Adam(pinn2.parameters(),lr=1e-3)
optimiser3 = torch.optim.Adam(pinn3.parameters(),lr=1e-3)
lambdas = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001, 0.000000001, 0.000000001]
d =0.1

i=0
u1_physics = pinn1(x_p) #(300,1)
du1dx = torch.autograd.grad(u1_physics, x_p, torch.ones_like(u1_physics), create_graph=True)[0]
u2_physics = pinn1(x_p) #(300,1)
du2dx = torch.autograd.grad(u2_physics, x_p, torch.ones_like(u2_physics), create_graph=True)[0]
u3_physics = pinn1(x_p)
du3dx = torch.autograd.grad(u2_physics, x_p, torch.ones_like(u2_physics), create_graph=True)[0]


physics_u1s = []
physics_u2s = []
physics_u3s = []
boundary_u1s = []
boundary_u2s = []
u1u2 = []
u1u3 = []
while i<=80000:
  #u1
  for j in range(30):
    optimiser1.zero_grad()
    u1_b_0 = pinn1(x_b_0)# (1, 1)
    u1_b_1 = pinn1(x_b_1)# (1, 1)
    u1_physics = pinn1(x_p) #(300,1)
    du1dx = torch.autograd.grad(u1_physics, x_p, torch.ones_like(u1_physics), create_graph=True)[0]

    u1_loss1 = (torch.squeeze(u1_b_0) - 1)**2
    u1_loss2 = torch.mean((u1_physics* du2dx.detach() + u2_physics.detach() * du1dx)**2)
    u1_loss3 =  torch.mean((u3_physics.detach() *du1dx + u1_physics*du3dx.detach() )**2)
    physics_loss_u1 = u1_loss3 + u1_loss2
    boundary_loss_u1 = u1_loss1

    u1_loss = lambdas[0]*u1_loss1 + lambdas[3]*(u1_loss2 + u1_loss3)
    u1_loss.backward()
    optimiser1.step()

    boundary_u1s.append(boundary_loss_u1.detach())
    physics_u1s.append(physics_loss_u1.detach())
    u1u2.append(torch.mean((u1_physics * u2_physics)).detach())
    i+=1
  #u2
  for k in range(30):
    optimiser2.zero_grad()
    u2_b_0 = pinn2(x_b_0)# (1, 1)
    u2_b_1 = pinn2(x_b_1)# (1, 1)
    u2_physics = pinn2(x_p) #(300,1)
    du2dx = torch.autograd.grad(u2_physics, x_p, torch.ones_like(u2_physics), create_graph=True)[0]

    u2_loss1 = (torch.squeeze(u2_b_0) - 1)**2
    u2_loss2 = (torch.squeeze(u2_b_1) - 20)**2
    u2_loss3 =  torch.mean((u1_physics.detach() * du2dx + u2_physics * du1dx.detach())**2)
    u2_loss4 =  torch.mean((u2_physics *du3dx.detach() - 2 * (u3_physics.detach() + 1/d)*du2dx + (1/d)*u3_physics.detach())**2)
    physics_loss_u2 = u2_loss3 + u2_loss4
    boundary_loss_u2 = u2_loss1 +  u2_loss2

    u2_loss = lambdas[0]*(u2_loss1+u2_loss2) + lambdas[3]*u2_loss3 + lambdas[3] * u2_loss4
    u2_loss.backward()
    optimiser2.step()

    boundary_u2s.append(boundary_loss_u2.detach())
    physics_u2s.append(physics_loss_u2.detach())
    u1u2.append(torch.mean((u1_physics * u2_physics)).detach())
    u1u3.append(torch.mean((u1_physics * u3_physics)).detach())
    i+=1
  #u3
  for l in range(30):
    optimiser3.zero_grad()
    u3_physics = pinn3(x_p)
    du3dx = torch.autograd.grad(u3_physics, x_p, torch.ones_like(u3_physics), create_graph=True)[0]

    u3_loss1 =  torch.mean((u3_physics*du1dx.detach() + u1_physics.detach()*du3dx )**2)
    u3_loss2 =  torch.mean((u2_physics.detach() *du3dx - 2 * (u3_physics + 1/d)*du2dx.detach() + (1/d)*u3_physics)**2)
    physics_loss_u3 = u3_loss1 + u3_loss2

    u3_loss = lambdas[3] * u3_loss1 + lambdas[7] * u3_loss2
    u3_loss.backward()
    optimiser3.step()

    physics_u3s.append(physics_loss_u3.detach())
    i+=1

  if i%900 == 0:
    pass

#plot u1,u2,u3
u1 = pinn1(x_t).detach()
u2 = pinn2(x_t).detach()
u3 = pinn3(x_t).detach()
plot_u1(x_p, x_b_0, x_b_1, x_t, u1, "u1 at iteration 40000")#)+ " for lambada = " + str(lambda1
plot_u2(x_p, x_b_0, x_b_1, x_t, u2, "u2 at iteration 40000")#+ " for lambada = " + str(lambda1))
plot_u3(x_p, x_b_0, x_b_1, x_t, u3, "u3 at iteration 40000") #+ " for lambada = " + str(lambda1))

'''
#plot du1, du2, du3
plot_output(x_p, x_b_0, x_b_1, x_t, du1dx.detach(), "Derivative of Output 1 (u1) at iteration" + str(i))# + " for lambada = " + str(lambda1))
plot_output(x_p, x_b_0, x_b_1, x_t, du2dx.detach(), "Derivative of Output 1 (u2) at iteration" + str(i)) # + " for lambada = " + str(lambda1))
plot_output(x_p, x_b_0, x_b_1, x_t, du3dx.detach(), "Derivative of Output 1 (u3) at iteration" + str(i)) #+ " for lambada = " + str(lambda1))
'''


plot_loss(physics_u1s, "physics loss for u1")
plot_loss(physics_u2s, "physics loss for u2")
plot_loss(physics_u3s, "physics loss for u3")
plot_loss(boundary_u1s, "boundary loss for u1")
plot_loss(boundary_u2s, "boundary loss for u2")
plot_loss(u1u2, "u1*u2")
plot_loss(u1u3, "u1*u3")
print(pinn3(x_b_0), " value of u3(0)")



mse_u1 = torch.mean(((numerical_u1) - (u1_physics)) ** 2).detach()
mse_u2 = torch.mean(((numerical_u2) - (u2_physics)) ** 2).detach()
mse_u3 = torch.mean(((numerical_u3) - (u3_physics)) ** 2).detach()

# Plot MSE for each variable
variables = ['u1', 'u2', 'u3']
mse_values = [mse_u1, mse_u2, mse_u3]

print(mse_values)
print(sum(mse_values))
plot_u1(x_p, x_b_0, x_b_1, x_t, u1, "u1 at iteration 80000")#)+ " for lambada = " + str(lambda1
plot_u2(x_p, x_b_0, x_b_1, x_t, u2, "u2 at iteration 80000")#+ " for lambada = " + str(lambda1))
plot_u3(x_p, x_b_0, x_b_1, x_t, u3, "u3 at iteration 80000") #+ " for lambada = " + str(lambda1))
plot_loss(physics_u1s, "")
plot_loss(physics_u2s, "")
plot_loss(physics_u3s, "")

"""simultaneous"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

# define boundary points, for the boundary loss
x_b_0 = torch.tensor(0.).view(-1,1).requires_grad_(True)
x_b_1 = torch.tensor(1.).view(-1,1).requires_grad_(True)
# define training points over the entire domain, for the physics loss
x_p = torch.linspace(0,1,301).view(-1,1).requires_grad_(True)
#test points
x_t = torch.linspace(0,1,301).view(-1,1)

pinn1 = FCN(1,1,32,3)
pinn2 = FCN(1,1,32,3)
pinn3 = FCN(1,1,32,3)

optimiser1 = torch.optim.Adam(pinn1.parameters(),lr=1e-3)
optimiser2 = torch.optim.Adam(pinn2.parameters(),lr=1e-3)
optimiser3 = torch.optim.Adam(pinn3.parameters(),lr=1e-3)
lambdas = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001, 0.000000001, 0.000000001]
d =0.01

i=0
u1_physics = pinn1(x_p) #(300,1)
du1dx = torch.autograd.grad(u1_physics, x_p, torch.ones_like(u1_physics), create_graph=True)[0]
u2_physics = pinn1(x_p) #(300,1)
du2dx = torch.autograd.grad(u2_physics, x_p, torch.ones_like(u2_physics), create_graph=True)[0]
u3_physics = pinn1(x_p)
du3dx = torch.autograd.grad(u2_physics, x_p, torch.ones_like(u2_physics), create_graph=True)[0]

physics_u1s = []
physics_u2s = []
physics_u3s = []
boundary_u1s = []
boundary_u2s = []
u1u2 = []
u1u3 = []

while i<=80001:
  #u1
  optimiser1.zero_grad()
  u1_b_0 = pinn1(x_b_0)# (1, 1)
  u1_b_1 = pinn1(x_b_1)# (1, 1)
  u1_physics = pinn1(x_p) #(300,1)
  du1dx = torch.autograd.grad(u1_physics, x_p, torch.ones_like(u1_physics), create_graph=True)[0]

  optimiser2.zero_grad()
  u2_b_0 = pinn2(x_b_0)# (1, 1)
  u2_b_1 = pinn2(x_b_1)# (1, 1)
  u2_physics = pinn2(x_p) #(300,1)
  du2dx = torch.autograd.grad(u2_physics, x_p, torch.ones_like(u2_physics), create_graph=True)[0]

  optimiser3.zero_grad()
  u3_physics = pinn3(x_p)
  du3dx = torch.autograd.grad(u3_physics, x_p, torch.ones_like(u3_physics), create_graph=True)[0]

  u1_loss1 = (torch.squeeze(u1_b_0) - 1)**2
  u1_loss2 = torch.mean((u1_physics* du2dx.detach() + u2_physics.detach() * du1dx)**2)
  u1_loss3 =  torch.mean((u3_physics.detach() *du1dx + u1_physics*du3dx.detach() )**2)

  physics_loss_u1 = u1_loss3 + u1_loss2
  boundary_loss_u1 = u1_loss1

  u2_loss1 = (torch.squeeze(u2_b_0) - 1)**2
  u2_loss2 = (torch.squeeze(u2_b_1) - 20)**2
  u2_loss3 =  torch.mean((u1_physics.detach() * du2dx + u2_physics * du1dx.detach())**2)
  u2_loss4 =  torch.mean((u2_physics *du3dx.detach() - 2 * (u3_physics.detach() + 1/d)*du2dx + (1/d)*u3_physics.detach())**2)

  physics_loss_u2 = u2_loss3 + u2_loss4
  boundary_loss_u2 = u2_loss1 +  u2_loss2

  u3_loss1 =  torch.mean((u3_physics*du1dx.detach() + u1_physics.detach()*du3dx )**2)
  u3_loss2 =  torch.mean((u2_physics.detach() *du3dx - 2 * (u3_physics + 1/d)*du2dx.detach() + (1/d)*u3_physics)**2)

  physics_loss_u3 = u3_loss1 + u3_loss2

  u1_loss = lambdas[0]*u1_loss1 + lambdas[3]*(u1_loss2 + u1_loss3)
  u1_loss.backward()
  optimiser1.step()

  u2_loss = lambdas[0]*(u2_loss1+u2_loss2) + lambdas[3]*u2_loss3 + lambdas[3] * u2_loss4
  u2_loss.backward()
  optimiser2.step()

  u3_loss = lambdas[3] * u3_loss1 + lambdas[6] * u3_loss2
  u3_loss.backward()
  optimiser3.step()


  boundary_u1s.append(boundary_loss_u1.detach())
  physics_u1s.append(physics_loss_u1.detach())
  u1u2.append(torch.mean((u1_physics * u2_physics)).detach())
  u1u3.append(torch.mean((u1_physics * u3_physics)).detach())
  boundary_u2s.append(boundary_loss_u2.detach())
  physics_u2s.append(physics_loss_u2.detach())
  physics_u3s.append(physics_loss_u3.detach())

  i+=1

print(u2_b_1)

#plot u1,u2,u3
u1 = pinn1(x_t).detach()
u2 = pinn2(x_t).detach()
u3 = pinn3(x_t).detach()
plot_u1(x_p, x_b_0, x_b_1, x_t, u1, "u1 at iteration 80000")#)+ " for lambada = " + str(lambda1
plot_u2(x_p, x_b_0, x_b_1, x_t, u2, "u2 at iteration 80000")#+ " for lambada = " + str(lambda1))
plot_u3(x_p, x_b_0, x_b_1, x_t, u3, "u3 at iteration 80000") #+ " for lambada = " + str(lambda1))
'''
    #plot du1, du2, du3
    plot_output(x_p, x_b_0, x_b_1, x_t, du1dx.detach(), "Derivative of Output 1 (u1) at iteration" + str(i))# + " for lambada = " + str(lambda1))
    plot_output(x_p, x_b_0, x_b_1, x_t, du2dx.detach(), "Derivative of Output 1 (u2) at iteration" + str(i)) # + " for lambada = " + str(lambda1))
    plot_output(x_p, x_b_0, x_b_1, x_t, du3dx.detach(), "Derivative of Output 1 (u3) at iteration" + str(i)) #+ " for lambada = " + str(lambda1))

'''
plot_loss(physics_u1s, "physics loss for u1")
plot_loss(physics_u2s, "physics loss for u2")
plot_loss(physics_u3s, "physics loss for u3")
plot_loss(boundary_u1s, "boundary loss for u1")
plot_loss(boundary_u2s, "boundary loss for u2")
plot_loss(u1u2, "u1*u2")
plot_loss(u1u3, "u1*u3")

mse_u1 = torch.mean(((numerical_u1) - (u1_physics)) ** 2).detach()
mse_u2 = torch.mean(((numerical_u2) - (u2_physics)) ** 2).detach()
mse_u3 = torch.mean(((numerical_u3) - (u3_physics)) ** 2).detach()

# Plot MSE for each variable
variables = ['u1', 'u2', 'u3']
mse_values = [mse_u1, mse_u2, mse_u3]
print(mse_values)
print(sum(mse_values))

plot_loss(physics_u1s, "")
plot_loss(physics_u2s, "")
plot_loss(physics_u3s, "")

"""SIMULTANEOUS TRAINING"""

print(torch.mean((u1_physics * u3_physics)).detach())
print(torch.mean((u1_physics * u2_physics)).detach())
print(pinn3(x_b_0), " value of u3(0)")
print(physics_loss_u1.detach())
print(physics_loss_u2.detach())
print(physics_loss_u3.detach())


# =============================================
# Hyperparameter Grid Search Implementation
# =============================================

import torch.nn as nn
import itertools
import os
import time

# Define hyperparameter grid
hyperparameter_grid = {
    'n_hidden': [16, 32, 64, 128, 256],  # Number of neurons per hidden layer
    'n_layers': [2, 3, 4, 5, 6],          # Number of hidden layers
    'learning_rate': [1e-2, 1e-3, 5e-4, 1e-4, 5e-5]  # Learning rate for Adam optimizer
}

# Create results directory if it doesn't exist
results_dir = 'hyperparam_results'
os.makedirs(results_dir, exist_ok=True)

# Function to train models with specific hyperparameters
def train_models_with_hyperparams(n_hidden, n_layers, learning_rate, max_iterations=100):
    print(f"\nTraining with hyperparameters: n_hidden={n_hidden}, n_layers={n_layers}, learning_rate={learning_rate}")
    
    # Create models with specified hyperparameters
    pinn1 = FCN(1, 1, n_hidden, n_layers)
    pinn2 = FCN(1, 1, n_hidden, n_layers)
    pinn3 = FCN(1, 1, n_hidden, n_layers)
    
    # Create optimizers with specified learning rate
    optimiser1 = torch.optim.Adam(pinn1.parameters(), lr=learning_rate)
    optimiser2 = torch.optim.Adam(pinn2.parameters(), lr=learning_rate)
    optimiser3 = torch.optim.Adam(pinn3.parameters(), lr=learning_rate)
    
    # Define boundary points and collocation points (same as original code)
    x_b_0 = torch.tensor(0.).view(-1,1).requires_grad_(True)
    x_b_1 = torch.tensor(1.).view(-1,1).requires_grad_(True)
    x_p = torch.linspace(0,1,301).view(-1,1).requires_grad_(True)
    x_t = torch.linspace(0,1,301).view(-1,1)
    
    # Initialize variables
    i = 0
    physics_u1s = []
    physics_u2s = []
    physics_u3s = []
    boundary_u1s = []
    boundary_u2s = []
    u1u2 = []
    u1u3 = []
    
    # Initial forward pass
    u1_physics = pinn1(x_p)
    du1dx = torch.autograd.grad(u1_physics, x_p, torch.ones_like(u1_physics), create_graph=True)[0]
    u2_physics = pinn2(x_p)
    du2dx = torch.autograd.grad(u2_physics, x_p, torch.ones_like(u2_physics), create_graph=True)[0]
    u3_physics = pinn3(x_p)
    du3dx = torch.autograd.grad(u3_physics, x_p, torch.ones_like(u3_physics), create_graph=True)[0]
    
    # For early stopping
    best_total_mse = float('inf')
    patience = 10
    patience_counter = 0
    best_models = None
    
    # Training loop (simultaneous training approach)
    start_time = time.time()
    
    while i <= max_iterations:
        # u1
        optimiser1.zero_grad()
        u1_b_0 = pinn1(x_b_0)
        u1_b_1 = pinn1(x_b_1)
        u1_physics = pinn1(x_p)
        du1dx = torch.autograd.grad(u1_physics, x_p, torch.ones_like(u1_physics), create_graph=True)[0]
        
        # u2
        optimiser2.zero_grad()
        u2_b_0 = pinn2(x_b_0)
        u2_b_1 = pinn2(x_b_1)
        u2_physics = pinn2(x_p)
        du2dx = torch.autograd.grad(u2_physics, x_p, torch.ones_like(u2_physics), create_graph=True)[0]
        
        # u3
        optimiser3.zero_grad()
        u3_physics = pinn3(x_p)
        du3dx = torch.autograd.grad(u3_physics, x_p, torch.ones_like(u3_physics), create_graph=True)[0]
        
        # Calculate losses for u1
        u1_loss1 = (torch.squeeze(u1_b_0) - 1)**2
        u1_loss2 = torch.mean((u1_physics* du2dx.detach() + u2_physics.detach() * du1dx)**2)
        u1_loss3 = torch.mean((u3_physics.detach() *du1dx + u1_physics*du3dx.detach() )**2)
        physics_loss_u1 = u1_loss3 + u1_loss2
        boundary_loss_u1 = u1_loss1
        u1_loss = lambdas[0]*u1_loss1 + lambdas[3]*(u1_loss2 + u1_loss3)
        
        # Calculate losses for u2
        u2_loss1 = (torch.squeeze(u2_b_0) - 1)**2
        u2_loss2 = (torch.squeeze(u2_b_1) - 20)**2
        u2_loss3 = torch.mean((u1_physics.detach() * du2dx + u2_physics * du1dx.detach())**2)
        u2_loss4 = torch.mean((u2_physics *du3dx.detach() - 2 * (u3_physics.detach() + 1/d)*du2dx + (1/d)*u3_physics.detach())**2)
        physics_loss_u2 = u2_loss3 + u2_loss4
        boundary_loss_u2 = u2_loss1 + u2_loss2
        u2_loss = lambdas[0]*(u2_loss1+u2_loss2) + lambdas[3]*u2_loss3 + lambdas[3] * u2_loss4
        
        # Calculate losses for u3
        u3_loss1 = torch.mean((u3_physics*du1dx.detach() + u1_physics.detach()*du3dx )**2)
        u3_loss2 = torch.mean((u2_physics.detach() *du3dx - 2 * (u3_physics + 1/d)*du2dx.detach() + (1/d)*u3_physics)**2)
        physics_loss_u3 = u3_loss1 + u3_loss2
        u3_loss = lambdas[3] * u3_loss1 + lambdas[6] * u3_loss2
        
        # Backpropagation and optimization step
        u1_loss.backward()
        optimiser1.step()
        
        u2_loss.backward()
        optimiser2.step()
        
        u3_loss.backward()
        optimiser3.step()
        
        # Store losses
        boundary_u1s.append(boundary_loss_u1.detach())
        physics_u1s.append(physics_loss_u1.detach())
        boundary_u2s.append(boundary_loss_u2.detach())
        physics_u2s.append(physics_loss_u2.detach())
        physics_u3s.append(physics_loss_u3.detach())
        u1u2.append(torch.mean((u1_physics * u2_physics)).detach())
        u1u3.append(torch.mean((u1_physics * u3_physics)).detach())
        
        # Calculate MSE against numerical solution every 100 iterations
        if i % 100 == 0:
            mse_u1 = torch.mean(((numerical_u1) - (u1_physics)) ** 2).detach()
            mse_u2 = torch.mean(((numerical_u2) - (u2_physics)) ** 2).detach()
            mse_u3 = torch.mean(((numerical_u3) - (u3_physics)) ** 2).detach()
            total_mse = mse_u1 + mse_u2 + mse_u3
            
            # Print progress
            if i % 1000 == 0:
                print(f"Iteration {i}, Total MSE: {total_mse.item():.6f}")
            
            # Early stopping check
            if total_mse < best_total_mse:
                best_total_mse = total_mse
                patience_counter = 0
                # Save best models
                best_models = {
                    'pinn1': copy.deepcopy(pinn1.state_dict()),
                    'pinn2': copy.deepcopy(pinn2.state_dict()),
                    'pinn3': copy.deepcopy(pinn3.state_dict())
                }
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"Early stopping at iteration {i}")
                    break
        
        i += 1
    
    # Calculate final MSE
    mse_u1 = torch.mean(((numerical_u1) - (u1_physics)) ** 2).detach()
    mse_u2 = torch.mean(((numerical_u2) - (u2_physics)) ** 2).detach()
    mse_u3 = torch.mean(((numerical_u3) - (u3_physics)) ** 2).detach()
    total_mse = mse_u1 + mse_u2 + mse_u3
    
    # Calculate training time
    training_time = time.time() - start_time
    
    # Return results
    return {
        'n_hidden': n_hidden,
        'n_layers': n_layers,
        'learning_rate': learning_rate,
        'mse_u1': mse_u1.item(),
        'mse_u2': mse_u2.item(),
        'mse_u3': mse_u3.item(),
        'total_mse': total_mse.item(),
        'iterations': i,
        'training_time': training_time,
        'best_models': best_models
    }

# Run grid search
def run_grid_search():
    print("Starting hyperparameter grid search...")
    
    # Generate all combinations of hyperparameters
    param_combinations = list(itertools.product(
        hyperparameter_grid['n_hidden'],
        hyperparameter_grid['n_layers'],
        hyperparameter_grid['learning_rate']
    ))
    
    print(f"Total number of combinations to test: {len(param_combinations)}")
    
    # Store results
    all_results = []
    best_result = None
    best_mse = float('inf')
    
    # Test each combination
    for idx, (n_hidden, n_layers, learning_rate) in enumerate(param_combinations):
        print(f"\nCombination {idx+1}/{len(param_combinations)}:")
        
        # Train models with current hyperparameters
        result = train_models_with_hyperparams(n_hidden, n_layers, learning_rate)
        all_results.append(result)
        
        # Check if this is the best result so far
        if result['total_mse'] < best_mse:
            best_mse = result['total_mse']
            best_result = result
            
            # Save best models so far
            if result['best_models'] is not None:
                # Create models with best hyperparameters
                best_pinn1 = FCN(1, 1, n_hidden, n_layers)
                best_pinn2 = FCN(1, 1, n_hidden, n_layers)
                best_pinn3 = FCN(1, 1, n_hidden, n_layers)
                
                # Load best weights
                best_pinn1.load_state_dict(result['best_models']['pinn1'])
                best_pinn2.load_state_dict(result['best_models']['pinn2'])
                best_pinn3.load_state_dict(result['best_models']['pinn3'])
                
                # Save models
                torch.save(best_pinn1.state_dict(), os.path.join(results_dir, 'best_pinn1.pt'))
                torch.save(best_pinn2.state_dict(), os.path.join(results_dir, 'best_pinn2.pt'))
                torch.save(best_pinn3.state_dict(), os.path.join(results_dir, 'best_pinn3.pt'))
    
    # Save all results to JSON file
    results_for_json = []
    for result in all_results:
        result_copy = result.copy()
        # Remove model state dictionaries as they can't be serialized to JSON
        if 'best_models' in result_copy:
            del result_copy['best_models']
        results_for_json.append(result_copy)
    
    with open(os.path.join(results_dir, 'grid_search_results.json'), 'w') as f:
        json.dump(results_for_json, f, indent=4)
    
    # Print best hyperparameters
    print("\n" + "=" * 50)
    print("Grid Search Complete!")
    print("=" * 50)
    print(f"Best hyperparameters:")
    print(f"  - Number of hidden neurons: {best_result['n_hidden']}")
    print(f"  - Number of layers: {best_result['n_layers']}")
    print(f"  - Learning rate: {best_result['learning_rate']}")
    print(f"Best total MSE: {best_result['total_mse']:.6f}")
    print(f"Individual MSEs: u1={best_result['mse_u1']:.6f}, u2={best_result['mse_u2']:.6f}, u3={best_result['mse_u3']:.6f}")
    print(f"Training time: {best_result['training_time']:.2f} seconds")
    print(f"Iterations: {best_result['iterations']}")
    print("\nBest models saved to 'hyperparam_results' directory")
    
    return best_result

# Run the grid search when this script is executed directly
if __name__ == "__main__":
    best_result = run_grid_search()
